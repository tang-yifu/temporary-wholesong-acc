{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import numpy as np\n",
    "from data_utils.pytorch_datasets.base_class import *\n",
    "# import data_utils\n",
    "# importlib.reload(data_utils)\n",
    "from data_utils.utils.read_file import read_data_no_acc\n",
    "from data_utils.read_pop909_data import analyze_pop909_dataset_without_acc\n",
    "from data_utils.pytorch_datasets import create_form_datasets, create_counterpoint_datasets, create_leadsheet_datasets, \\\n",
    "    create_accompaniment_datasets\n",
    "from data_utils.utils.song_analyzer import LanguageExtractor\n",
    "from data_utils.pytorch_datasets.const import LANGUAGE_DATASET_PARAMS, AUTOREG_PARAMS, SHIFT_HIGH_T, SHIFT_LOW_T, SHIFT_HIGH_V, SHIFT_LOW_V\n",
    "from data_utils.pytorch_datasets.form_dataset import FormDataset\n",
    "from data_utils.pytorch_datasets.counterpoint_dataset import CounterpointDataset\n",
    "from data_utils.pytorch_datasets.leadsheet_dataset import LeadSheetDataset\n",
    "\n",
    "\n",
    "class ExtractFormCounterpointLeadsheet:\n",
    "    def __init__(self, analyses):\n",
    "        self.n_channels_frm = 8\n",
    "        self.n_channels_ctp = 10\n",
    "        self.n_channels_lsh = 12\n",
    "        self.h_frm = 16\n",
    "        self.h_ctp = 128\n",
    "        self.h_lsh = 128\n",
    "\n",
    "        self.frm_max_l = [analysis['languages']['form']['key_roll'].shape[1] for analysis in analyses]\n",
    "        self.ctp_max_l = [analysis['languages']['counterpoint']['red_mel_roll'].shape[1] for analysis in analyses]\n",
    "        self.lsh_max_l = [analysis['languages']['lead_sheet']['mel_roll'].shape[1] for analysis in analyses]\n",
    "\n",
    "        form_langs = [analysis['languages']['form'] for analysis in analyses]\n",
    "        ctpt_langs = [analysis['languages']['counterpoint'] for analysis in analyses]\n",
    "        ldsht_langs = [analysis['languages']['lead_sheet'] for analysis in analyses]\n",
    "\n",
    "        self.min_mel_pitches = [analysis['min_mel_pitch'] for analysis in analyses]\n",
    "        self.max_mel_pitches = [analysis['max_mel_pitch'] for analysis in analyses]\n",
    "\n",
    "        self.nbpms = [analysis['nbpm'] for analysis in analyses]\n",
    "        self.nspbs = [analysis['nspb'] for analysis in analyses]\n",
    "        self.song_names = [analysis['name'] for analysis in analyses]\n",
    "\n",
    "        self.key_rolls = [form_lang['key_roll'] for form_lang in form_langs]\n",
    "        self.key_rolls_ctp = [expand_roll(roll, nbpm) for roll, nbpm in zip(self.key_rolls, self.nbpms)]\n",
    "        self.key_rolls_lsh = [expand_roll(roll, nbpm * nspb)\n",
    "                          for roll, nbpm, nspb in zip(self.key_rolls, self.nbpms, self.nspbs)]\n",
    "\n",
    "        self.phrase_rolls = [form_lang['phrase_roll'][:, :, np.newaxis] for form_lang in form_langs]\n",
    "        self.phrase_rolls_ctp = [expand_roll(roll, nbpm) for roll, nbpm in zip(self.phrase_rolls, self.nbpms)]\n",
    "        self.phrase_rolls_lsh = [expand_roll(roll, nbpm * nspb)\n",
    "                             for roll, nbpm, nspb in zip(self.phrase_rolls, self.nbpms, self.nspbs)]\n",
    "\n",
    "        self.red_mel_rolls = [ctpt_lang['red_mel_roll'] for ctpt_lang in ctpt_langs]\n",
    "        self.red_mel_rolls_lsh = [expand_roll(roll, nspb, contain_onset=True)\n",
    "                              for roll, nspb in zip(self.red_mel_rolls, self.nspbs)]\n",
    "        \n",
    "        self.red_chd_rolls = [ctpt_lang['red_chd_roll'] for ctpt_lang in ctpt_langs]\n",
    "        self.red_chd_rolls_lsh = [expand_roll(roll, nspb, contain_onset=True)\n",
    "                              for roll, nspb in zip(self.red_chd_rolls, self.nspbs)]\n",
    "\n",
    "        self.mel_rolls = [ldsht_lang['mel_roll'] for ldsht_lang in ldsht_langs]\n",
    "        self.chd_rolls = [ldsht_lang['chd_roll'] for ldsht_lang in ldsht_langs]\n",
    "        self.chd_rolls_lsh = [expand_roll(roll, nspb, contain_onset=True)\n",
    "                          for roll, nspb in zip(self.chd_rolls, self.nspbs)]\n",
    "\n",
    "\n",
    "    def get_data_sample_frm(self, song_id=0, start_id=0, end_id=None):\n",
    "\n",
    "        # store_key\n",
    "        if self.key_rolls is not None:\n",
    "            key_roll = self.key_rolls[song_id]\n",
    "            self._key = np.roll(key_roll, shift=0, axis=-1)\n",
    "        \n",
    "        # store_phrase\n",
    "        if self.phrase_rolls is not None:\n",
    "            self._phrase = self.phrase_rolls[song_id]\n",
    "\n",
    "        if end_id is None:\n",
    "            end_id = self.frm_max_l[song_id]\n",
    "\n",
    "        img = self.lang_to_img_frm(start_id, end_id=end_id, tgt_lgth=end_id-start_id)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def get_data_sample_ctp(self, song_id=0, start_id=0, end_id=None):\n",
    "        nbpm = self.nbpms[song_id]\n",
    "\n",
    "        # store_key\n",
    "        if self.key_rolls_ctp is not None:\n",
    "            key_roll = self.key_rolls_ctp[song_id]\n",
    "            self._key = np.roll(key_roll, shift=0, axis=-1)\n",
    "        \n",
    "        # store_phrase\n",
    "        if self.phrase_rolls_ctp is not None:\n",
    "            self._phrase = self.phrase_rolls_ctp[song_id]\n",
    "\n",
    "        # store_red_mel\n",
    "        if self.red_mel_rolls is not None:\n",
    "            red_mel_roll = self.red_mel_rolls[song_id]\n",
    "            self._red_mel = np.roll(red_mel_roll, shift=0, axis=-1)\n",
    "\n",
    "        # store_red_chd\n",
    "        if self.red_chd_rolls is not None:\n",
    "            red_chd_roll = self.red_chd_rolls[song_id]\n",
    "            self._red_chd = np.roll(red_chd_roll, shift=0, axis=-1)\n",
    "\n",
    "        if end_id is None:\n",
    "            end_id = self.ctp_max_l[song_id]\n",
    "        img = self.lang_to_img_ctp(start_id, end_id, tgt_lgth=end_id-start_id)\n",
    "        return img\n",
    "        \n",
    "    def get_data_sample_lsh(self, song_id=0, start_id=0, end_id=None):\n",
    "        nbpm, nspb = self.nbpms[song_id], self.nspbs[song_id]\n",
    "\n",
    "        # self.store_key(song_id, pitch_shift)\n",
    "        if self.key_rolls_lsh is not None:\n",
    "            key_roll = self.key_rolls_lsh[song_id]\n",
    "            self._key = np.roll(key_roll, shift=0, axis=-1)\n",
    "\n",
    "        # self.store_phrase(song_id)\n",
    "        if self.phrase_rolls_lsh is not None:\n",
    "            self._phrase = self.phrase_rolls_lsh[song_id]\n",
    "\n",
    "        # self.store_red_mel(song_id, pitch_shift)\n",
    "        if self.red_mel_rolls_lsh is not None:\n",
    "            red_mel_roll = self.red_mel_rolls_lsh[song_id]\n",
    "            self._red_mel = np.roll(red_mel_roll, shift=0, axis=-1)\n",
    "\n",
    "        # self.store_red_chd(song_id, pitch_shift)\n",
    "        if self.red_chd_rolls_lsh is not None:\n",
    "            red_chd_roll = self.red_chd_rolls_lsh[song_id]\n",
    "            self._red_chd = np.roll(red_chd_roll, shift=0, axis=-1)\n",
    "\n",
    "        # self.store_mel(song_id, pitch_shift)\n",
    "        if self.mel_rolls is not None:\n",
    "            mel_roll = self.mel_rolls[song_id]\n",
    "            self._mel = np.roll(mel_roll, shift=0, axis=-1)\n",
    "\n",
    "        # self.store_chd(song_id, pitch_shift)\n",
    "        if self.chd_rolls_lsh is not None:\n",
    "            chd_roll = self.chd_rolls_lsh[song_id]\n",
    "            self._chd = np.roll(chd_roll, shift=0, axis=-1)\n",
    "\n",
    "        if end_id is None:\n",
    "            end_id = self.lsh_max_l[song_id]\n",
    "\n",
    "        img = self.lang_to_img_lsh(start_id, end_id=end_id, tgt_lgth=end_id-start_id)\n",
    "\n",
    "        return img\n",
    "\n",
    "    def lang_to_img_frm(self, start_id, end_id, tgt_lgth=None):\n",
    "        key_roll = self._key[:, start_id: end_id]  # (2, L, 12)\n",
    "        phrase_roll = self._phrase[:, start_id: end_id]  # (6, L, 1)\n",
    "\n",
    "        # actual_l = self._key.shape[1]\n",
    "\n",
    "        # to output image\n",
    "        if tgt_lgth is None:\n",
    "            tgt_lgth = end_id - start_id\n",
    "        img = np.zeros((self.n_channels_frm, tgt_lgth, self.h_frm), dtype=np.float32)\n",
    "        img[0: 2, 0: tgt_lgth, 0: 12] = key_roll\n",
    "        img[2: 8, 0: tgt_lgth] = phrase_roll\n",
    "\n",
    "        return img\n",
    "    \n",
    "    def lang_to_img_ctp(self, start_id, end_id, tgt_lgth=None):\n",
    "        print(tgt_lgth)\n",
    "        key_roll = self._key[:, start_id: end_id]  # (2, L, 12)\n",
    "        phrase_roll = self._phrase[:, start_id: end_id]  # (6, L, 1)\n",
    "        red_mel_roll = self._red_mel[:, start_id: end_id]  # (2, L, 128)\n",
    "        red_chd_roll = self._red_chd[:, start_id: end_id]  # (6, L, 12)\n",
    "\n",
    "        # actual_l = key_roll.shape[1]\n",
    "\n",
    "        # to output image\n",
    "        if tgt_lgth is None:\n",
    "            tgt_lgth = self._key.shape[1] - start_id\n",
    "        img = np.zeros((self.n_channels_ctp, tgt_lgth, 132), dtype=np.float32)\n",
    "        print(tgt_lgth, red_mel_roll.shape)\n",
    "        img[0: 2, 0: tgt_lgth, 0: 128] = red_mel_roll\n",
    "        img[0: 2, 0: tgt_lgth, 36: 48] = red_chd_roll[2: 4]\n",
    "        img[0: 2, 0: tgt_lgth, 24: 36] = red_chd_roll[4: 6]\n",
    "\n",
    "        img[4: 10, 0: tgt_lgth] = phrase_roll\n",
    "\n",
    "        img = img.reshape((self.n_channels_ctp, tgt_lgth, 11, 12))\n",
    "        img[2: 4, 0: tgt_lgth] = key_roll[:, :, np.newaxis]\n",
    "        img = img.reshape((self.n_channels_ctp, tgt_lgth, 132))\n",
    "        return img[:, :, 0: self.h_ctp]\n",
    "    \n",
    "    def lang_to_img_lsh(self, start_id, end_id, tgt_lgth=None):\n",
    "        key_roll = self._key[:, start_id: end_id]  # (2, L, 12)\n",
    "        phrase_roll = self._phrase[:, start_id: end_id]  # (6, L, 1)\n",
    "        red_mel_roll = self._red_mel[:, start_id: end_id]  # (2, L, 128)\n",
    "        red_chd_roll = self._red_chd[:, start_id: end_id]  # (6, L, 12)\n",
    "        mel_roll = self._mel[:, start_id: end_id]\n",
    "        chd_roll = self._chd[:, start_id: end_id]\n",
    "\n",
    "        # actual_l = key_roll.shape[1]\n",
    "\n",
    "        # to output image\n",
    "        if tgt_lgth is None:\n",
    "            tgt_lgth = end_id - start_id\n",
    "        img = np.zeros((self.n_channels_lsh, tgt_lgth, 132), dtype=np.float32)\n",
    "        img[0: 2, 0: tgt_lgth, 0: 128] = mel_roll\n",
    "        img[0: 2, 0: tgt_lgth, 36: 48] = chd_roll[2: 4]\n",
    "        img[0: 2, 0: tgt_lgth, 24: 36] = chd_roll[4: 6]\n",
    "\n",
    "        img[2: 4, 0: tgt_lgth, 0: 128] = red_mel_roll\n",
    "        img[2: 4, 0: tgt_lgth, 36: 48] = red_chd_roll[2: 4]\n",
    "        img[2: 4, 0: tgt_lgth, 24: 36] = red_chd_roll[4: 6]\n",
    "\n",
    "        img[6: 12, 0: tgt_lgth] = phrase_roll\n",
    "\n",
    "        img = img.reshape((self.n_channels_lsh, tgt_lgth, 11, 12))\n",
    "        img[4: 6, 0: tgt_lgth] = key_roll[:, :, np.newaxis]\n",
    "        img = img.reshape((self.n_channels_lsh, tgt_lgth, 132))\n",
    "        return img[:, :, 0: self.h_lsh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BEAT_PER_MEASURE = 4\n",
    "song_data = read_data_no_acc(\"preprocessing/external_data/pig_mel\", num_beat_per_measure=BEAT_PER_MEASURE, num_step_per_beat=4,\n",
    "              clean_chord_unit=None, song_name=None, label=1)\n",
    "lang_extractor = LanguageExtractor(song_data)\n",
    "hie_lang = lang_extractor.analyze_without_acc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 23, 12)\n",
      "(2, 92, 128)\n",
      "(2, 368, 128)\n"
     ]
    }
   ],
   "source": [
    "print(hie_lang['languages']['form']['key_roll'].shape)\n",
    "print(hie_lang['languages']['counterpoint']['red_mel_roll'].shape)\n",
    "print(hie_lang['languages']['lead_sheet']['mel_roll'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "32 (2, 32, 128)\n"
     ]
    }
   ],
   "source": [
    "mega_data = ExtractFormCounterpointLeadsheet([hie_lang])\n",
    "start_measure = 0\n",
    "end_measure = 8\n",
    "frm = mega_data.get_data_sample_frm(start_id=start_measure, end_id=end_measure)\n",
    "ctp = mega_data.get_data_sample_ctp(start_id=start_measure, end_id=end_measure*BEAT_PER_MEASURE)\n",
    "lsh = mega_data.get_data_sample_lsh(start_id=start_measure, end_id=end_measure*BEAT_PER_MEASURE*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "frm = np.expand_dims(frm, axis=0)\n",
    "ctp = np.expand_dims(ctp, axis=0)\n",
    "lsh = np.expand_dims(lsh, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate whole song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default default default default\n",
      "Description of the experiment is: m0-v-default-default\n",
      "m1-v-default-default\n",
      "m2-v-default-default\n",
      "m3-v-default-default\n"
     ]
    }
   ],
   "source": [
    "# whole_song_gen_notebook.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "from experiments.whole_song_gen import WholeSongGeneration\n",
    "import torch\n",
    "\n",
    "# Default model folders and demo directory\n",
    "DEFAULT_FRM_MODEL_FOLDER = 'results_default/frm---/v-default'\n",
    "DEFAULT_CTP_MODEL_FOLDER = 'results_default/ctp-a-b-/v-default'\n",
    "DEFAULT_LSH_MODEL_FOLDER = 'results_default/lsh-a-b-/v-default'\n",
    "DEFAULT_ACC_MODEL_FOLDER = 'results_default/acc-a-b-/v-default'\n",
    "DEFAULT_DEMO_DIR = 'demo'\n",
    "\n",
    "# Set the argument values directly\n",
    "args = {\n",
    "    'demo_dir': DEFAULT_DEMO_DIR,\n",
    "    'mpath0': DEFAULT_FRM_MODEL_FOLDER,\n",
    "    'mid0': 'default',\n",
    "    'mpath1': DEFAULT_CTP_MODEL_FOLDER,\n",
    "    'mid1': 'default',\n",
    "    'mpath2': DEFAULT_LSH_MODEL_FOLDER,\n",
    "    'mid2': 'default',\n",
    "    'mpath3': DEFAULT_ACC_MODEL_FOLDER,\n",
    "    'mid3': 'default',\n",
    "    'nsample': 1,\n",
    "    'pstring': None,\n",
    "    'nbpm': 4,\n",
    "    'key': 0,\n",
    "    'minor': False,\n",
    "    'debug': False\n",
    "}\n",
    "\n",
    "# Check available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Initialize the whole song generation pipeline\n",
    "whole_song_expr = WholeSongGeneration.init_pipeline(\n",
    "    frm_model_folder=args['mpath0'],\n",
    "    ctp_model_folder=args['mpath1'],\n",
    "    lsh_model_folder=args['mpath2'],\n",
    "    acc_model_folder=args['mpath3'],\n",
    "    frm_model_id=args['mid0'],\n",
    "    ctp_model_id=args['mid1'],\n",
    "    lsh_model_id=args['mid2'],\n",
    "    acc_model_id=args['mid3'],\n",
    "    debug_mode=args['debug'],\n",
    "    device=device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whole_song_expr.frm_op.data_params = {'max_l': 8, 'h': 16, 'n_channel': 8, 'cur_channel': 8}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from inference.utils import quantize_generated_form_batch, specify_form\n",
    "\n",
    "n_sample=args['nsample']\n",
    "nbpm=BEAT_PER_MEASURE\n",
    "nspb=4  # assuming nspb is a constant value\n",
    "phrase_string=\"B34\"\n",
    "key=args['key']\n",
    "is_major=args['minor']\n",
    "demo_dir=args['demo_dir']\n",
    "bpm = 110\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Form generation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Sample...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the song: 8, phrase_label:\n",
      "0: i1\n",
      "1: i7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## form generation\n",
    "print(\"Form generation...\")\n",
    "frm_canvas, slices, gen_max_l = whole_song_expr.frm_op.create_canvas(n_sample=1, prompt=None)\n",
    "frm_1 = whole_song_expr.frm_op.generation(frm_canvas, slices, gen_max_l, quantize=False, n_sample=1)\n",
    "frm_2, lengths, phrase_labels = quantize_generated_form_batch(frm_1)\n",
    "print(f\"Length of the song: {lengths[0]}, phrase_label:\\n{phrase_labels[0]}\")\n",
    "frm = frm_2[:, :, 0: lengths[0]]\n",
    "phrase_string = phrase_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counterpoint generation...\n",
      "Number of iterations: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Sample...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ctp generation\n",
    "print(\"Counterpoint generation...\")\n",
    "background_cond = whole_song_expr.ctp_op.expand_background(frm, nbpm)\n",
    "\n",
    "ctp_canvas, slices, gen_max_l = \\\n",
    "    whole_song_expr.ctp_op.create_canvas(background_cond, n_sample, nbpm, None, whole_song_expr.random_n_autoreg)\n",
    "print(f\"Number of iterations: {len(slices)}\")\n",
    "ctp = whole_song_expr.ctp_op.generation(ctp_canvas, slices, gen_max_l)\n",
    "ctp = np.stack(ctp, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lead Sheet generation...\n",
      "Number of iterations: 4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Sample...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Lead Sheet generation\n",
    "print(\"Lead Sheet generation...\")\n",
    "background_cond = whole_song_expr.lsh_op.expand_background(ctp, nspb)\n",
    "lsh_canvas, slices, gen_max_l = \\\n",
    "    whole_song_expr.lsh_op.create_canvas(background_cond, n_sample, nbpm, nspb, None, whole_song_expr.random_n_autoreg)\n",
    "print(f\"Number of iterations: {len(slices)}\")\n",
    "lsh = whole_song_expr.lsh_op.generation(lsh_canvas, slices, gen_max_l)\n",
    "lsh = np.stack(lsh, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accompaniment generation...\n",
      "Number of iterations: 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"overflow-x: scroll;\">Sample...</pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Accompaniment generation\n",
    "print(\"Accompaniment generation...\")\n",
    "acc_canvas, slices, gen_max_l = \\\n",
    "    whole_song_expr.acc_op.create_canvas(lsh, n_sample, nbpm, nspb, None, whole_song_expr.random_n_autoreg)\n",
    "print(f\"Number of iterations: {len(slices)}\")\n",
    "acc = whole_song_expr.acc_op.generation(acc_canvas, slices, gen_max_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_file = whole_song_expr.output(acc, phrase_string, key, is_major, demo_dir, bpm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
